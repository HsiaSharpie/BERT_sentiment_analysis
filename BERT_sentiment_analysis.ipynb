{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm_notebook, trange\n",
    "from IPython.display import clear_output\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Dataset - step1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    " 資料預處理步驟 ->\n",
    " 1. 資料的 row 是以 reviews, label 輪流交替出現，且會有資料沒對齊的狀況，若沒對齊 -> 刪除。\n",
    " 2. 為了節省記憶體空間，把 sentence length 超過 200 的句子刪除。\n",
    "    length -> 使用 BERT 斷詞後的 WordPiece token 去計算個數。\n",
    " 3. 輸出 dataframe，其中 columns 分別為單一句子，整串句子concatenation 以及 label。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "def data_processing(file):\n",
    "    # read and parse tsv file\n",
    "    reviews = []\n",
    "    label = []\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        rows = csv.reader(f, delimiter='\\t', quotechar='\"')\n",
    "        for index, row in enumerate(rows):\n",
    "            if index % 2 == 0:\n",
    "                reviews.append(row)\n",
    "            else:\n",
    "                label.append(row)\n",
    "\n",
    "        for i, j in zip(reviews, label):\n",
    "            if len(i) != len(j):\n",
    "                unequal_idx = reviews.index(i)\n",
    "                reviews.pop(unequal_idx)\n",
    "                label.pop(unequal_idx)\n",
    "\n",
    "        # check whether the length of reviews & sentiment are equal\n",
    "        for i, j in zip(reviews, label):\n",
    "            assert len(i) == len(j)\n",
    "    \n",
    "    whole_reviews = [] # store the whole row into list\n",
    "    reviews_len = [] # store the length of the row\n",
    "    \n",
    "    for review in reviews:\n",
    "        whole_reviews.append(\"\".join(review))\n",
    "        reviews_len.append(len(review))\n",
    "    \n",
    "    # For reducing memory, give up the sentence with longer than the length of 200.\n",
    "    row_reviews = []\n",
    "    for idx, review in enumerate(whole_reviews):\n",
    "        tokens = tokenizer.tokenize(review)\n",
    "        if len(tokens) > 200:\n",
    "            temp = []\n",
    "            temp.append(review[:200])\n",
    "            row_reviews.append(temp*reviews_len[idx])\n",
    "        else:\n",
    "            temp = []\n",
    "            temp.append(review)\n",
    "            row_reviews.append(temp*reviews_len[idx])\n",
    "    \n",
    "    # from 2d to 1d\n",
    "    reviews = list(itertools.chain.from_iterable(reviews))\n",
    "    row_reviews = list(itertools.chain.from_iterable(row_reviews))\n",
    "    label = list(itertools.chain.from_iterable(label))\n",
    "    \n",
    "    # return as pandas dataframe\n",
    "    df_bert = pd.DataFrame({\n",
    "        'reviews': reviews,\n",
    "        'row_reviews': row_reviews,\n",
    "        'label': label,\n",
    "    })\n",
    "    \n",
    "    return df_bert\n",
    "\n",
    "file_name = 'training_set.tsv'\n",
    "df_bert = data_processing(file_name)\n",
    "df_bert.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess sentence words - step2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    " 資料預處理步驟 ->\n",
    " 1. 移除 punctuations\n",
    " 2. 移除不需要的字詞、空白、相對不重要的英文數字序號。\n",
    " 3. 替換一些不常見的字詞。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'zhon'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5c8dd4990c58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mzhon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhanzi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpunctuation\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mpunctuation\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'zhon'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "import string\n",
    "\n",
    "punctuation =  punctuation + string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'punctuation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-407c87bc06cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mremove_punctuation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[{}]'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_bert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreviews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_bert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreviews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremove_punctuation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_init_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mremove_compiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'^[0-9]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'punctuation' is not defined"
     ]
    }
   ],
   "source": [
    "remove_punctuation = re.compile(r'[{}]'.format(punctuation))\n",
    "df_bert.reviews = df_bert.reviews.apply(lambda x : re.sub(remove_punctuation, \"\", x))\n",
    "\n",
    "def remove_init_num(x):\n",
    "    remove_compiler = re.compile(r'^[0-9]')\n",
    "    if len(x) > 1 :\n",
    "        if (not x[1].isdigit()) :\n",
    "            return re.sub(remove_compiler, '', x)\n",
    "    return x\n",
    "\n",
    "def remove_space(x):\n",
    "    return re.sub(r'\\s+', '', x)\n",
    "\n",
    "def sub_unk_eng(x, sub_word):\n",
    "    sub_compiler = re.compile(r'[A-Za-z][0-9A-Za-z]+')\n",
    "    return re.sub(sub_compiler, sub_word, x)\n",
    "\n",
    "\n",
    "df_bert.reviews = df_bert.reviews.apply(remove_space)\n",
    "df_bert.reviews = df_bert.reviews.apply(remove_init_num)\n",
    "df_bert['sub_reviews'] = df_bert['reviews'].apply(lambda x : sub_unk_eng(x, '产品'))\n",
    "df_bert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>row_reviews</th>\n",
       "      <th>label</th>\n",
       "      <th>sub_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>千呼万唤始出来</td>\n",
       "      <td>千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>千呼万唤始出来</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>尼康的APSC小相机终于发布了</td>\n",
       "      <td>千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>尼康的产品小相机终于发布了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COOLPIXA你怎么看呢</td>\n",
       "      <td>千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>产品你怎么看呢</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>我看尼康是挤牙膏挤惯了啊</td>\n",
       "      <td>千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>我看尼康是挤牙膏挤惯了啊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>外观既没有V1时尚</td>\n",
       "      <td>千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...</td>\n",
       "      <td>negative</td>\n",
       "      <td>外观既没有产品时尚</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           reviews                                        row_reviews     label    sub_reviews\n",
       "0          千呼万唤始出来  千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...   neutral        千呼万唤始出来\n",
       "1  尼康的APSC小相机终于发布了  千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...   neutral  尼康的产品小相机终于发布了\n",
       "2    COOLPIXA你怎么看呢  千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...   neutral        产品你怎么看呢\n",
       "3     我看尼康是挤牙膏挤惯了啊  千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...   neutral   我看尼康是挤牙膏挤惯了啊\n",
       "4        外观既没有V1时尚  千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...  negative      外观既没有产品时尚"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bert.row_reviews = df_bert.row_reviews.apply(remove_space)\n",
    "df_bert.row_reviews = df_bert.row_reviews.apply(remove_init_num)\n",
    "df_bert.row_reviews = df_bert.row_reviews.apply(lambda x : sub_unk_eng(x, '产品'))\n",
    "df_bert.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Dataset - step3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "---> 替換部分 BERT tokenize 後為 [\"UNK\"] 之字眼。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
    "\n",
    "# 取得此預訓練模型所使用的 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Note:\n",
    "查看有哪些 token 被轉換為 [\"UNK\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_token_ids(x):\n",
    "    return tokenizer.tokenize(x)\n",
    "    \n",
    "df_bert['reviews_tokens'] = df_bert['sub_reviews'].apply(to_token_ids)\n",
    "reviews_tokens = df_bert['reviews_tokens'].tolist()\n",
    "\n",
    "unk_idx = []\n",
    "for idx, review in enumerate(reviews_tokens):\n",
    "    for tok in review:\n",
    "        if tok == \"[UNK]\":\n",
    "            unk_idx.append(idx)\n",
    "            \n",
    "df_unk_bert = df_bert.iloc[unk_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Note:\n",
    "替換部分 [\"UNK\"] 為對應的產品。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {\n",
    "    '1D': '佳能相機',\n",
    "    '5D': '佳能相機',\n",
    "    '6D': '佳能相機',\n",
    "    '7D': '佳能相機',\n",
    "    '40D': '佳能相機',\n",
    "    '50D': '佳能相機',\n",
    "    '60D': '佳能相機',\n",
    "    '70D': '佳能相機',\n",
    "    '350D': '佳能相機',\n",
    "    '500D': '佳能相機',\n",
    "    '550D': '佳能相機',\n",
    "    '650D': '佳能相機',\n",
    "    'P系列': '尼康相機',\n",
    "    'D300': '尼康相機',\n",
    "    'D200': '尼康相機',\n",
    "    'ＬＣＤ': '液晶顯示器',\n",
    "    '徕卡M': '徕卡相機',\n",
    "    '徕卡X': '徕卡相機',\n",
    "    '1855Ⅱ': '鏡頭',\n",
    "    'K卡口': '卡口',\n",
    "    '廋廋': '瘦瘦',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac3b4a09c714106b497c071d5b9a1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da561d1c4ddb4882a987a81ed9200fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_reviews</th>\n",
       "      <th>row_reviews</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>千呼万唤始出来</td>\n",
       "      <td>千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>尼康的产品小相机终于发布了</td>\n",
       "      <td>千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>产品你怎么看呢</td>\n",
       "      <td>千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>我看尼康是挤牙膏挤惯了啊</td>\n",
       "      <td>千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>外观既没有产品时尚</td>\n",
       "      <td>千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sub_reviews                                        row_reviews     label\n",
       "0        千呼万唤始出来  千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...   neutral\n",
       "1  尼康的产品小相机终于发布了  千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...   neutral\n",
       "2        产品你怎么看呢  千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...   neutral\n",
       "3   我看尼康是挤牙膏挤惯了啊  千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...   neutral\n",
       "4      外观既没有产品时尚  千呼万唤始出来，尼康的产品小相机终于发布了，产品.你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1...  negative"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_reviews = df_bert['sub_reviews'].tolist()\n",
    "row_reviews = df_bert['row_reviews'].tolist()\n",
    "\n",
    "def sub_unknown(sub_dict, sub_list, df, col):\n",
    "    new_sub_reviews = {}\n",
    "    for idx, review in tqdm_notebook(enumerate(sub_reviews)):\n",
    "        for key, value in sub_dict.items():\n",
    "            if key in review:\n",
    "                match_str = '(' + key + ')'\n",
    "                split_tok = re.split(match_str, review)\n",
    "\n",
    "                # 替換[\"UNK\"]\n",
    "                split_tok = [sub_dict[tok] if tok in sub_dict else tok for tok in split_tok]\n",
    "                sub_rev = \"\".join(split_tok)\n",
    "                new_sub_reviews[idx] = sub_rev\n",
    "\n",
    "    for key, value in new_sub_reviews.items():\n",
    "        df_bert.loc[key, col] = value\n",
    "        \n",
    "        \n",
    "sub_unknown(replace_dict, sub_reviews, df_bert, 'sub_reviews')\n",
    "sub_unknown(replace_dict, row_reviews, df_bert, 'row_reviews')\n",
    "\n",
    "del df_bert['reviews']\n",
    "del df_bert['reviews_tokens']\n",
    "\n",
    "cols = df_bert.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df_bert = df_bert[cols]\n",
    "df_bert.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the proportion of different labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral      1.283216\n",
       "positive     7.062500\n",
       "negative    12.639821\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative = 0\n",
    "# neutral = 1\n",
    "# positive = 2\n",
    "1 / (df_bert.label.value_counts() / df_bert.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "我們僅有 training dataset，但要在 12/25當天實際預測 testing dataset。\n",
    "故預先切分以調整測試。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df_bert, test_df_bert = train_test_split(df_bert, test_size=0.2, random_state=42)\n",
    "\n",
    "# os.mkdir('data')\n",
    "train_df_bert.to_csv('train.tsv', sep='\\t', index=False)\n",
    "test_df_bert.to_csv('test.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建構一個用來讀取訓練 / 測試集的 Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv(r\"\" + mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        # test\n",
    "        if self.mode == \"test\": \n",
    "            \n",
    "            review_a, review_b = self.df.iloc[idx, :2].values\n",
    "            label_tensor = None # 在 test mode 中，label設定為 None 以用於預測\n",
    "        \n",
    "        # train\n",
    "        else:\n",
    "            review_a, review_b, label = self.df.iloc[idx, :].values\n",
    "            label_id = self.label_map[label]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "        \n",
    "        \n",
    "        # 建立句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = [\"[CLS]\"] # 起始 token\n",
    "        tokens_a = self.tokenizer.tokenize(review_a)\n",
    "        word_pieces += tokens_a + [\"[SEP]\"]\n",
    "        len_a = len(tokens_a)\n",
    "        \n",
    "        # 第二個句子的 BERT tokens\n",
    "        tokens_b = self.tokenizer.tokenize(review_b)\n",
    "        word_pieces += tokens_b + [\"[SEP]\"]\n",
    "        len_b = len(word_pieces) - len_a\n",
    "        \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
    "        segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
    "                                        dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # train mode 有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    \n",
    "    # test mode 無 labels\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 DataLoader 拆分 trainiset 成數個 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化\n",
    "trainset = ReviewsDataset(\"train\", tokenizer=tokenizer)\n",
    "BATCH_SIZE = 16\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下游任務學習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-chinese'\n",
    "NUM_LABELS = 3\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建構預測 label 之 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm_notebook(dataloader):\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None] \n",
    "            \n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(\n",
    "                    input_ids=tokens_tensors, \n",
    "                    token_type_ids=segments_tensors, \n",
    "                    attention_mask=masks_tensors\n",
    "            )\n",
    "\n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1) # 1 -> 對列取 max\n",
    "            \n",
    "            # 計算訓練集的精準度\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0) # 紀錄目前訓練之總data數目\n",
    "                correct += (pred == labels).sum().item()\n",
    "            \n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "        \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建構 BERT Model 並查看在未訓練狀況下之 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "model.train()\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(),\n",
    "                                weight_decay = 1e-6,\n",
    "                                lr=1e-5)\n",
    "\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "criterion = nn.NLLLoss(weight=torch.tensor([12.84, 1.27, 7.14]).to(device))\n",
    "EPOCHS = 12\n",
    "for epoch in tqdm_notebook(range(EPOCHS)):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = criterion(m(outputs[1]), labels)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('[Epoch %d] loss: %.3f, acc: %.3f' %\n",
    "          (epoch + 1, running_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 DataLoader 拆分 testset 成數個 batch 並進行預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "testset = ReviewsDataset(\"test\", tokenizer=tokenizer)\n",
    "testloader = DataLoader(testset, batch_size=64, \n",
    "                        collate_fn=create_mini_batch,\n",
    "                        shuffle=False)\n",
    "\n",
    "# 預測測試集\n",
    "predictions = get_predictions(model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抓出 ground truth，並預測訓練後之結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth\n",
    "def get_test_accuracy(pred, truth):\n",
    "    # predictions: gpu -> cpu\n",
    "    corr_count = (pred.cpu() == truth).sum().item()\n",
    "    return corr_count / len(truth)\n",
    "\n",
    "label_mapping = {\n",
    "    'negative': 0,\n",
    "    'neutral': 1,\n",
    "    'positive': 2\n",
    "    } \n",
    "ground_truth = torch.tensor([label_mapping[i] for i in testset.df.iloc[:, 1].values])\n",
    "\n",
    "get_test_accuracy(predictions, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 因資料之 label 為不平衡資料，故也查看 precision/recall/f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "test_Y = ground_truth\n",
    "pred_Y = predictions.cpu()\n",
    "\n",
    "accuracy = accuracy_score(test_Y, pred_Y)\n",
    "precision = precision_score(test_Y, pred_Y, average='macro')\n",
    "recall = recall_score(test_Y, pred_Y, average='macro')\n",
    "fscore = f1_score(test_Y, pred_Y, average='macro')\n",
    "\n",
    "print(\"Accuracy: %g\\tPrecision: %g\\tRecall: %g\\tF-score: %g\" % (\n",
    "    accuracy, precision, recall, fscore))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
